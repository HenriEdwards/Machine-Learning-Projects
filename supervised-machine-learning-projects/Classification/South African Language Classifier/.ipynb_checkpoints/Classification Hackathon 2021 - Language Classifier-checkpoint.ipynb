{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54239f4b",
   "metadata": {},
   "source": [
    "## Classification Hackathon 2021 - Language Classifier\n",
    "Henri Edwards - Explore Data Science - Class of July 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78e8abd",
   "metadata": {},
   "source": [
    "### 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76915c",
   "metadata": {},
   "source": [
    "South Africa is a multicultural society that is characterised by its rich linguistic diversity. Language is an indispensable tool that can be used to deepen democracy and also contribute to the social, cultural, intellectual, economic and political life of the South African society.\n",
    "\n",
    "The country is multilingual with 11 official languages, each of which is guaranteed equal status. Most South Africans are multilingual and able to speak at least two or more of the official languages.\n",
    "\n",
    "With such a multilingual population, it is only obvious that our systems and devices also communicate in multi-languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63926d9d",
   "metadata": {},
   "source": [
    "### 2. Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5180e0ac",
   "metadata": {},
   "source": [
    "In this challenge, you will take text which is in any of South Africa's 11 Official languages and identify which language the text is in. This is an example of NLP's Language Identification, the task of determining the natural language that a piece of text is written in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71100c23",
   "metadata": {},
   "source": [
    "### 3. Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1828dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Prerequisites\n",
    "import sys\n",
    "import nltk\n",
    "\n",
    "# Exploratory Data Analysis\n",
    "import re\n",
    "import ast\n",
    "import time\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Preprocessing\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Classification Model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Performance Evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9c60cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore spammy warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb273a",
   "metadata": {},
   "source": [
    "### 4. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df262afb",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train_set.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-b163f47770df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_set.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_set.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1216\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1217\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1218\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    787\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    790\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_set.csv'"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_set.csv')\n",
    "test_df = pd.read_csv('test_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View train_df\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa532af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View test_df\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af9fc06",
   "metadata": {},
   "source": [
    "### 5. Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):    \n",
    "    \n",
    "    \"\"\"Function that takes in input text, removes stop words, transforms text to lowercase, removes punctuation and hyperlinks\"\"\"    \n",
    "\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    \n",
    "    text = text.lower() # Changes input text to lowercase for better cleaning\n",
    "    text =  ' '.join(re.sub(\"([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",text).split()) # Remove punctuation\n",
    "    text = re.sub(\"https?:\\/\\/\\S+\", \"\", text) # Remove hyper links\n",
    "    text = ' '.join(word for word in text.split() if word not in stopwords_list) # Remove StopWords\n",
    "    return text    \n",
    "    \n",
    "train_df['text'] = train_df['text'].apply(cleaning) # Apply function to train_df\n",
    "test_df['text'] = test_df['text'].apply(cleaning) # Apply function to test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fab6dba",
   "metadata": {},
   "source": [
    "**Tokenisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d0e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    \n",
    "    \"\"\"Function tokenizes input string, and output tokenized text\"\"\"\n",
    "    \n",
    "    text = word_tokenize(text)\n",
    "    return text\n",
    "\n",
    "# Applies function and creates a new feature with function output\n",
    "train_df['tokens'] = train_df['text'].apply(cleaning)\n",
    "test_df['tokens'] = test_df['text'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108dae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view tokenized feature\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9381b64",
   "metadata": {},
   "source": [
    "**Stemming**\n",
    "\n",
    "Stemming outperformed lemmitization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ebc3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(text, stemmer):\n",
    "    \n",
    "    \"\"\"Function performs stemming on a tokenized feature, and outputs a stemmed feature\"\"\"\n",
    "    \n",
    "    return [stemmer.stem(word) for word in text]\n",
    "\n",
    "train_df['stem'] = train_df['tokens'].apply(stemming, args=(stemmer, )) # Apply function to train_df\n",
    "test_df['stem'] = test_df['tokens'].apply(stemming, args=(stemmer, )) # Apply function to test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e68fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ListToSentence(text):\n",
    "    \n",
    "    \"\"\"Function converts lists to strings\"\"\"\n",
    "    \n",
    "    return ' '.join(word for word in text)\n",
    "\n",
    "train_df['tokens'] = train_df['tokens'].apply(ListToSentence) # Apply function to train_df\n",
    "train_df['stem'] = train_df['stem'].apply(ListToSentence) # Apply function to train_df\n",
    "\n",
    "test_df['tokens'] = test_df['tokens'].apply(ListToSentence) # Apply function to test_df\n",
    "test_df['stem'] = test_df['stem'].apply(ListToSentence) # Apply function to test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1135e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57228a10",
   "metadata": {},
   "source": [
    "### 6. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a90ea1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all objects\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f568ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return total values to predict\n",
    "print('Total languages to predict: '+ str(train_df['lang_id'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ab1a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return target values to predict\n",
    "train_df['lang_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f44824c",
   "metadata": {},
   "source": [
    "### 7. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf52f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign independent variable to X and dependent variable y\n",
    "X = train_df['stem']\n",
    "y = train_df['lang_id']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize X\n",
    "vect = CountVectorizer()\n",
    "X = vect.fit_transform(X)\n",
    "\n",
    "# Splitting train_df into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.01, random_state=42) #using most of the data for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf845b11",
   "metadata": {},
   "source": [
    "#### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2418d31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign algorithm to clf\n",
    "clf = MultinomialNB(alpha=1)\n",
    "\n",
    "# Fit data to MNB model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b83f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on training data\n",
    "y_pred = clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b5c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report\n",
    "print(metrics.classification_report(y_val, y_pred))\n",
    "\n",
    "# Print f1 score\n",
    "print('F1_score: ',round(metrics.f1_score(y_val, y_pred, average = 'macro'),8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cdb965",
   "metadata": {},
   "source": [
    "### 8. Model Performance\n",
    "Performing hyperparameter tuning using the function GridSearchCV to increase prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527ada51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View available hyperparameters for MNB algorithm\n",
    "MultinomialNB().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8de68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter selection for gridsearch\n",
    "alphs = [0.0005, 0.0025, 0.005]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb196c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference the hyperparameter selection\n",
    "param_grid = {'alpha': alphs}\n",
    "\n",
    "# Assign algorithm to MNB\n",
    "MNB = MultinomialNB()\n",
    "\n",
    "# Assign gridsearch to grid_MNB\n",
    "grid_MNB = GridSearchCV(MNB, param_grid, scoring='f1')\n",
    "\n",
    "# Fit model to training data using Gridsearch\n",
    "grid_MNB.fit(X_train, y_train)\n",
    "\n",
    "# Get best performing hyperparameters\n",
    "grid_MNB.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on train data using best performing hyperparameters\n",
    "y_pred = grid_MNB.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f98769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification metrix\n",
    "print(metrics.classification_report(y_val, y_pred))\n",
    "\n",
    "# Print F1 score\n",
    "print('F1_score: ',round(metrics.f1_score(y_val, y_pred, average = 'macro'),8)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6bc31b",
   "metadata": {},
   "source": [
    "### 9. Submission\n",
    "For kaggle submission only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a4e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test data\n",
    "pred_test_data = grid_MNB.predict(vect.transform(test_df['stem']))\n",
    "pred_df = pd.DataFrame(data=test_df['index'], columns=['index'])\n",
    "pred_df.insert(1, 'lang_id', pred_test_data, allow_duplicates=False)\n",
    "pred_df.to_csv(path_or_buf='Submission.csv', index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
